name: Conversation ID System Tests

# Trigger the workflow on push to main/production branches, pull requests, and manual dispatch
on:
  push:
    branches: [main, production, develop]
    paths:
      - 'api/src/chatbot/**'
      - 'tests/chatbot-conversation-id-tests.js'
      - 'tests/ci-cd-conversation-id-runner.js'
      - '.github/workflows/conversation-id-tests.yml'
  pull_request:
    branches: [main, production, develop]
    paths:
      - 'api/src/chatbot/**'
      - 'tests/chatbot-conversation-id-tests.js'
      - 'tests/ci-cd-conversation-id-runner.js'
  schedule:
    # Run daily at 2 AM UTC to catch any regressions
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - quick
          - performance-only
      fail_fast:
        description: 'Exit on first failure'
        required: false
        default: false
        type: boolean
      verbose_output:
        description: 'Enable verbose test output'
        required: false
        default: false
        type: boolean

# Define environment variables
env:
  NODE_ENV: test
  CI: true
  CONVERSATION_ID_CI_MODE: true

jobs:
  # Job 1: Test Environment Setup and Validation
  setup-validation:
    name: Environment Setup & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      node-version: ${{ steps.setup.outputs.node-version }}
      test-environment: ${{ steps.validation.outputs.environment }}
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          # Fetch full history for accurate git operations
          fetch-depth: 0

      - name: Setup Node.js
        id: setup
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'tests/package-lock.json'

      - name: Install Dependencies
        working-directory: tests
        run: |
          npm ci --only=prod --no-audit
          echo "‚úÖ Dependencies installed successfully"

      - name: Validate Test Environment
        id: validation
        working-directory: tests
        run: |
          echo "üîç Validating test environment..."
          
          # Check Node.js version
          node_version=$(node --version)
          echo "Node.js version: $node_version"
          
          # Check required files exist
          if [[ ! -f "chatbot-conversation-id-tests.js" ]]; then
            echo "‚ùå Test file not found"
            exit 1
          fi
          
          if [[ ! -f "ci-cd-conversation-id-runner.js" ]]; then
            echo "‚ùå CI/CD runner not found"
            exit 1
          fi
          
          # Check permissions
          if [[ ! -r "chatbot-conversation-id-tests.js" ]]; then
            echo "‚ùå Cannot read test file"
            exit 1
          fi
          
          # Set output
          echo "environment=validated" >> $GITHUB_OUTPUT
          echo "‚úÖ Environment validation completed"

  # Job 2: Core Conversation ID Tests
  conversation-id-tests:
    name: Conversation ID System Tests
    runs-on: ubuntu-latest
    needs: setup-validation
    timeout-minutes: 15
    strategy:
      matrix:
        test-config:
          - name: "Standard Test Suite"
            mode: "standard"
            timeout: 300000
          - name: "Performance Focus"
            mode: "performance"
            timeout: 600000
          - name: "Edge Cases Focus"
            mode: "edge-cases"
            timeout: 300000
      fail-fast: ${{ github.event.inputs.fail_fast == 'true' }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'tests/package-lock.json'

      - name: Install Dependencies
        working-directory: tests
        run: npm ci --only=prod --no-audit

      - name: Create Reports Directory
        working-directory: tests
        run: |
          mkdir -p reports
          chmod 755 reports

      - name: Execute Conversation ID Tests
        id: test-execution
        working-directory: tests
        env:
          CONVERSATION_ID_TEST_MODE: ${{ matrix.test-config.mode }}
          CONVERSATION_ID_TIMEOUT: ${{ matrix.test-config.timeout }}
        run: |
          echo "üöÄ Running ${{ matrix.test-config.name }}..."
          
          # Set test parameters based on inputs
          TEST_ARGS=""
          if [[ "${{ github.event.inputs.fail_fast }}" == "true" ]]; then
            TEST_ARGS="$TEST_ARGS --fail-fast"
          fi
          if [[ "${{ github.event.inputs.verbose_output }}" == "true" ]]; then
            TEST_ARGS="$TEST_ARGS --verbose"
          fi
          
          # Execute tests with timeout
          timeout ${{ matrix.test-config.timeout }}s node ci-cd-conversation-id-runner.js $TEST_ARGS --timeout ${{ matrix.test-config.timeout }} || {
            exit_code=$?
            echo "test-exit-code=$exit_code" >> $GITHUB_OUTPUT
            
            if [[ $exit_code == 124 ]]; then
              echo "‚ùå Test execution timed out"
              echo "test-result=TIMEOUT" >> $GITHUB_OUTPUT
            elif [[ $exit_code == 1 ]]; then
              echo "‚ö†Ô∏è Some tests failed"
              echo "test-result=PARTIAL_FAILURE" >> $GITHUB_OUTPUT
            elif [[ $exit_code == 2 ]]; then
              echo "‚ùå Test execution error"
              echo "test-result=EXECUTION_ERROR" >> $GITHUB_OUTPUT
            elif [[ $exit_code == 3 ]]; then
              echo "‚ùå Configuration error"
              echo "test-result=CONFIG_ERROR" >> $GITHUB_OUTPUT
            fi
            
            exit $exit_code
          }
          
          echo "‚úÖ Tests completed successfully"
          echo "test-result=SUCCESS" >> $GITHUB_OUTPUT
          echo "test-exit-code=0" >> $GITHUB_OUTPUT

      - name: Upload Test Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: conversation-id-test-reports-${{ matrix.test-config.mode }}
          path: |
            tests/reports/conversation-id-*.json
            tests/reports/conversation-id-*.html
            tests/reports/conversation-id-*.xml
            tests/reports/conversation-id-*.tap
          retention-days: 30
          if-no-files-found: warn

      - name: Parse Test Results
        if: always()
        id: parse-results
        working-directory: tests
        run: |
          # Find the latest JSON report
          LATEST_REPORT=$(ls -t reports/conversation-id-cicd-*.json 2>/dev/null | head -1)
          
          if [[ -f "$LATEST_REPORT" ]]; then
            echo "üìä Parsing test results from: $LATEST_REPORT"
            
            # Extract key metrics using jq if available, otherwise use grep/sed
            if command -v jq &> /dev/null; then
              SCORE=$(jq -r '.analysis.score // 0' "$LATEST_REPORT")
              GRADE=$(jq -r '.analysis.grade // "F"' "$LATEST_REPORT")
              STATUS=$(jq -r '.analysis.overallStatus // "UNKNOWN"' "$LATEST_REPORT")
              DURATION=$(jq -r '.totalDuration // 0' "$LATEST_REPORT")
            else
              # Fallback parsing without jq
              SCORE=$(grep -o '"score":[^,]*' "$LATEST_REPORT" | head -1 | cut -d':' -f2 | tr -d ' "')
              GRADE=$(grep -o '"grade":"[^"]*"' "$LATEST_REPORT" | head -1 | cut -d':' -f2 | tr -d ' "')
              STATUS=$(grep -o '"overallStatus":"[^"]*"' "$LATEST_REPORT" | head -1 | cut -d':' -f2 | tr -d ' "')
              DURATION=$(grep -o '"totalDuration":[^,]*' "$LATEST_REPORT" | head -1 | cut -d':' -f2 | tr -d ' "')
            fi
            
            echo "test-score=${SCORE:-0}" >> $GITHUB_OUTPUT
            echo "test-grade=${GRADE:-F}" >> $GITHUB_OUTPUT
            echo "test-status=${STATUS:-UNKNOWN}" >> $GITHUB_OUTPUT
            echo "test-duration=${DURATION:-0}" >> $GITHUB_OUTPUT
            
            echo "üìà Test Score: ${SCORE}%"
            echo "üìä Test Grade: ${GRADE}"
            echo "‚è±Ô∏è Duration: ${DURATION}ms"
          else
            echo "‚ö†Ô∏è No test report found"
            echo "test-score=0" >> $GITHUB_OUTPUT
            echo "test-grade=F" >> $GITHUB_OUTPUT
            echo "test-status=NO_REPORT" >> $GITHUB_OUTPUT
            echo "test-duration=0" >> $GITHUB_OUTPUT
          fi

      - name: Publish Test Results
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Conversation ID Tests (${{ matrix.test-config.name }})
          path: tests/reports/conversation-id-junit-*.xml
          reporter: java-junit
          fail-on-error: false

  # Job 3: Test Results Analysis and Quality Gates
  test-analysis:
    name: Test Analysis & Quality Gates
    runs-on: ubuntu-latest
    needs: conversation-id-tests
    if: always()
    timeout-minutes: 5

    steps:
      - name: Download Test Reports
        uses: actions/download-artifact@v4
        with:
          path: test-results
          merge-multiple: true

      - name: Analyze Test Results
        id: analysis
        run: |
          echo "üîç Analyzing test results across all test configurations..."
          
          # Initialize counters
          total_configs=0
          successful_configs=0
          total_score=0
          
          # Process each test configuration
          for report in test-results/conversation-id-cicd-*.json; do
            if [[ -f "$report" ]]; then
              total_configs=$((total_configs + 1))
              
              # Extract score (simple grep-based parsing)
              score=$(grep -o '"score":[^,]*' "$report" | head -1 | cut -d':' -f2 | tr -d ' "' || echo "0")
              total_score=$((total_score + score))
              
              # Check if this configuration passed (score >= 95)
              if [[ $(echo "$score >= 95" | bc 2>/dev/null || echo "0") == "1" ]]; then
                successful_configs=$((successful_configs + 1))
              fi
              
              echo "üìä Configuration result: Score=$score%"
            fi
          done
          
          if [[ $total_configs -gt 0 ]]; then
            average_score=$((total_score / total_configs))
            success_rate=$(( (successful_configs * 100) / total_configs ))
            
            echo "overall-score=$average_score" >> $GITHUB_OUTPUT
            echo "success-rate=$success_rate" >> $GITHUB_OUTPUT
            echo "successful-configs=$successful_configs" >> $GITHUB_OUTPUT
            echo "total-configs=$total_configs" >> $GITHUB_OUTPUT
            
            echo "üìà Overall Score: $average_score%"
            echo "‚úÖ Success Rate: $success_rate% ($successful_configs/$total_configs)"
          else
            echo "‚ö†Ô∏è No test results found for analysis"
            echo "overall-score=0" >> $GITHUB_OUTPUT
            echo "success-rate=0" >> $GITHUB_OUTPUT
          fi

      - name: Quality Gate Check
        id: quality-gate
        run: |
          OVERALL_SCORE="${{ steps.analysis.outputs.overall-score }}"
          SUCCESS_RATE="${{ steps.analysis.outputs.success-rate }}"
          
          echo "üö™ Quality Gate Check:"
          echo "  Average Score: $OVERALL_SCORE%"
          echo "  Success Rate: $SUCCESS_RATE%"
          
          # Quality gate thresholds
          MIN_SCORE=90
          MIN_SUCCESS_RATE=80
          
          QUALITY_PASSED=true
          
          if [[ $(echo "$OVERALL_SCORE < $MIN_SCORE" | bc 2>/dev/null || echo "1") == "1" ]]; then
            echo "‚ùå Quality gate failed: Score $OVERALL_SCORE% is below minimum $MIN_SCORE%"
            QUALITY_PASSED=false
          fi
          
          if [[ $(echo "$SUCCESS_RATE < $MIN_SUCCESS_RATE" | bc 2>/dev/null || echo "1") == "1" ]]; then
            echo "‚ùå Quality gate failed: Success rate $SUCCESS_RATE% is below minimum $MIN_SUCCESS_RATE%"
            QUALITY_PASSED=false
          fi
          
          echo "quality-passed=$QUALITY_PASSED" >> $GITHUB_OUTPUT
          
          if [[ "$QUALITY_PASSED" == "true" ]]; then
            echo "‚úÖ Quality gate passed!"
            exit 0
          else
            echo "‚ùå Quality gate failed!"
            exit 1
          fi

      - name: Create Summary
        if: always()
        run: |
          echo "## ü§ñ Conversation ID Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Overall Score** | ${{ steps.analysis.outputs.overall-score }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| **Success Rate** | ${{ steps.analysis.outputs.success-rate }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| **Configurations** | ${{ steps.analysis.outputs.successful-configs }}/${{ steps.analysis.outputs.total-configs }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Quality Gate** | ${{ steps.quality-gate.outputs.quality-passed == 'true' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Workflow** | ${{ github.workflow }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Commit** | \`${{ github.sha }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| **Branch** | \`${{ github.ref_name }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ steps.quality-gate.outputs.quality-passed }}" != "true" ]]; then
            echo "### ‚ö†Ô∏è Quality Gate Failed" >> $GITHUB_STEP_SUMMARY
            echo "The conversation ID system tests did not meet the required quality thresholds." >> $GITHUB_STEP_SUMMARY
            echo "Please review the test results and address any issues before merging." >> $GITHUB_STEP_SUMMARY
          fi

  # Job 4: Slack/Teams Notification (Optional)
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [conversation-id-tests, test-analysis]
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/production')
    timeout-minutes: 2

    steps:
      - name: Notification Setup
        id: setup
        run: |
          # Determine notification color and status
          if [[ "${{ needs.test-analysis.outputs.quality-passed }}" == "true" ]]; then
            echo "status=SUCCESS" >> $GITHUB_OUTPUT
            echo "color=good" >> $GITHUB_OUTPUT
            echo "emoji=‚úÖ" >> $GITHUB_OUTPUT
          else
            echo "status=FAILED" >> $GITHUB_OUTPUT
            echo "color=danger" >> $GITHUB_OUTPUT
            echo "emoji=‚ùå" >> $GITHUB_OUTPUT
          fi

      - name: Send Slack Notification
        if: env.SLACK_WEBHOOK_URL != ''
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ steps.setup.outputs.status }}
          author_name: IT-ERA CI/CD
          title: "Conversation ID Tests: ${{ steps.setup.outputs.status }}"
          text: |
            ${{ steps.setup.outputs.emoji }} **Conversation ID System Tests**
            
            **Branch:** `${{ github.ref_name }}`
            **Commit:** `${{ github.sha }}`
            **Score:** ${{ needs.test-analysis.outputs.overall-score }}%
            **Success Rate:** ${{ needs.test-analysis.outputs.success-rate }}%
            
            [View Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}